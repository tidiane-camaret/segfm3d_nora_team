{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff879d13b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)  # Disable gradient calculation for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/repos/segfm3d-nora')\n",
    "sys.path.append('/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/repos/nnunetv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"DATA_DIR\": \"/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_incontext/SegFM3D/data\",\n",
    "    \"RESULTS_DIR\": \"/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_incontext/SegFM3D/results\",\n",
    "    \"SAM_CKPT_PATH\": \"/nfs/norasys/notebooks/camaret/SAM-Med3D/ckpt/sam_med3d_turbo_bbox_cvpr.pth\",\n",
    "    \"SAM_REPO_DIR\": \"/nfs/norasys/notebooks/camaret/SAM-Med3D\",\n",
    "    \"ONNX_MODEL_PATH\": \"/nfs/norasys/notebooks/camaret/model_inference/models/sammed3d.onnx\",\n",
    "    \"NNINT_CKPT_DIR\": \"/nfs/norasys/notebooks/camaret/model_checkpoints/nnint\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from nnInteractive.inference.inference_session import nnInteractiveInferenceSession\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_for_network = {\n",
    "    \"arch_class_name\": \"dynamic_network_architectures.architectures.unet.ResidualEncoderUNet\",\n",
    "    \"arch_kwargs\": {\n",
    "        \"n_stages\": 6,\n",
    "        \"features_per_stage\": [32, 64, 128, 256, 320, 320],\n",
    "        \"conv_op\": \"torch.nn.modules.conv.Conv3d\",\n",
    "        \"kernel_sizes\": [\n",
    "            [3, 3, 3],\n",
    "            [3, 3, 3],\n",
    "            [3, 3, 3],\n",
    "            [3, 3, 3],\n",
    "            [3, 3, 3],\n",
    "            [3, 3, 3],\n",
    "        ],\n",
    "        \"strides\": [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "        \"n_blocks_per_stage\": [1, 3, 4, 6, 6, 6],\n",
    "        \"n_conv_per_stage_decoder\": [1, 1, 1, 1, 1],\n",
    "        \"conv_bias\": True,\n",
    "        \"norm_op\": \"torch.nn.modules.instancenorm.InstanceNorm3d\",\n",
    "        \"norm_op_kwargs\": {\"eps\": 1e-05, \"affine\": True},\n",
    "        \"dropout_op\": None,\n",
    "        \"dropout_op_kwargs\": None,\n",
    "        \"nonlin\": \"torch.nn.LeakyReLU\",\n",
    "        \"nonlin_kwargs\": {\"inplace\": True},\n",
    "    },\n",
    "    \"arch_kwargs_req_import\": [\"conv_op\", \"norm_op\", \"dropout_op\", \"nonlin\"],\n",
    "    \"input_channels\": 8,\n",
    "    \"output_channels\": 2,\n",
    "    \"allow_init\": True,\n",
    "    \"deep_supervision\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "network = get_network_from_plans(**args_for_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 102,355,818\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "network = get_network_from_plans(**args_for_network).cuda()\n",
    "orig_checkpoint_dir = '/nfs/norasys/notebooks/camaret/model_checkpoints/nnint/nnInteractive_v1.0/'\n",
    "newer_checkpoint_dir = \"/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/data/model-checkpoints/new-dataset/\"\n",
    "if os.path.exists(newer_checkpoint_dir):\n",
    "    chkpt_dict = torch.load(os.path.join(newer_checkpoint_dir, 'fold_0/checkpoint_final.pth'), weights_only=False)\n",
    "else:\n",
    "    chkpt_dict = torch.load(os.path.join(orig_checkpoint_dir, 'fold_0/checkpoint_final.pth'), weights_only=False)\n",
    "\n",
    "network.load_state_dict(chkpt_dict['network_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.training.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.npzdataset import NPZNoCacheDataset\n",
    "from src.training.transforms import CropToNonzeroImage\n",
    "from src.training.transforms import NormalizeSingleImageTransformNumpy\n",
    "from src.training.transforms import ChooseClass\n",
    "from src.training.transforms import CropAndAddBBox\n",
    "from src.training.transforms import AddEmptyChan\n",
    "from src.training.transforms import WrapMONAI\n",
    "from src.training.transforms import MergeImageAndInteractionAndSeg\n",
    "from batchgeneratorsv2.transforms.utils.deep_supervision_downsampling import (\n",
    "    DownsampleSegForDSTransform,\n",
    ")\n",
    "from monai.transforms import Resized\n",
    "from batchgeneratorsv2.transforms.utils.compose import ComposeTransforms\n",
    "\n",
    "only_2d_bbox = False\n",
    "\n",
    "test_transforms = [\n",
    "            CropToNonzeroImage(),\n",
    "            NormalizeSingleImageTransformNumpy(),\n",
    "            ChooseClass(),\n",
    "            CropAndAddBBox(),\n",
    "            AddEmptyChan(),\n",
    "            WrapMONAI(Resized(\n",
    "                keys=[\"image\", \"segmentation\", \"bbox_chan\"], spatial_size=(192, 192, 192),\n",
    "                mode=[\"trilinear\", \"nearest\", \"nearest\"]\n",
    "            )),\n",
    "            MergeImageAndInteractionAndSeg(),\n",
    "        ]\n",
    "test_transform = ComposeTransforms(test_transforms)\n",
    "train_transforms = test_transforms + [\n",
    "            DownsampleSegForDSTransform(\n",
    "            ds_scales=[\n",
    "                [1.0, 1.0, 1.0],\n",
    "                [0.5, 0.5, 0.5],\n",
    "                [0.25, 0.25, 0.25],\n",
    "                [0.125, 0.125, 0.125],\n",
    "                [0.0625, 0.0625, 0.0625],\n",
    "            ])]\n",
    "train_transform =ComposeTransforms(train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.npzdataset import NPZPytorchDataset, NPZDataset\n",
    "from src.training.utils import fast_ids_to_deterministic_floats\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "parent_folder = '/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_incontext/SegFM3D/data/3D_train_npz_random_10percent_16G/'\n",
    "\n",
    "to_exclude = ['Ultrasound/US_Low-limb-Leg/US_Low-limb-Leg07.npz',\n",
    "       'Ultrasound/US_Low-limb-Leg/US_Low-limb-Leg24.npz',\n",
    "       'Ultrasound/US_Low-limb-Leg/US_Low-limb-Leg37.npz',\n",
    "       'CT/CT_Abdomen1K/CT_Abdomen1K_Case_00931.npz',\n",
    "       'Microscopy/Microscopy_cremi/Microscopy_cremi_002_sc.npz',\n",
    "       'Microscopy/Microscopy_cremi/Microscopy_cremi_000_sc.npz',\n",
    "       'Microscopy/Microscopy_cremi/Microscopy_cremi_001_sc.npz']\n",
    "all_npz_file_paths = glob(os.path.join(parent_folder, '**/*.npz'), recursive=True)\n",
    "\n",
    "cleaned_file_paths = np.array(sorted([f for f in all_npz_file_paths if not any(e in f for e in to_exclude)]))\n",
    "\n",
    "train_mask = fast_ids_to_deterministic_floats(cleaned_file_paths) < 0.8\n",
    "train_files = cleaned_file_paths[train_mask]\n",
    "test_files = cleaned_file_paths[~train_mask]\n",
    "train_set = NPZNoCacheDataset(train_files, transform=train_transform)\n",
    "test_set = NPZNoCacheDataset(test_files, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = int(os.environ['SLURM_CPUS_PER_TASK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=n_jobs,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=n_jobs,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with custom forward print shape\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm, trange\n",
    "import torch\n",
    "from src.training.model_wrap import ModelPrevSegAndClickWrapper\n",
    "from src.training.utils import defer_keysignal_with_grad\n",
    "\n",
    "wrapped_network = ModelPrevSegAndClickWrapper(network, n_max_clicks=1)\n",
    "\n",
    "optim_network = torch.optim.AdamW(\n",
    "    wrapped_network.parameters(), lr=1e-4, weight_decay=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path /nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/data/model-checkpoints/new-dataset/ exists already, fine\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import shutil\n",
    "out_dir = '/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/data/model-checkpoints/new-dataset/'#'/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/data/model-checkpoints/try_from_nb_all_data_competition_bbox/'\n",
    "try:\n",
    "    shutil.copytree(orig_checkpoint_dir, out_dir)\n",
    "except FileExistsError:\n",
    "    print(f\"path {out_dir} exists already, fine\")\n",
    "trained_chkpt_path = os.path.join(out_dir, 'fold_0/checkpoint_final.pth')\n",
    "assert os.path.exists(trained_chkpt_path)\n",
    "\n",
    "\n",
    "trained_chkpt = deepcopy(chkpt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_batch_size = 16\n",
    "n_batches_accumulate = accumulate_batch_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::alias encountered 312 time(s)\n",
      "Unsupported operator aten::leaky_relu_ encountered 116 time(s)\n",
      "Unsupported operator aten::add_ encountered 52 time(s)\n",
      "Unsupported operator aten::avg_pool3d encountered 10 time(s)\n",
      "Unsupported operator aten::diff encountered 1 time(s)\n",
      "Unsupported operator aten::ne encountered 1 time(s)\n",
      "Unsupported operator aten::meshgrid encountered 1 time(s)\n",
      "Unsupported operator aten::sub encountered 1 time(s)\n",
      "Unsupported operator aten::sum encountered 2 time(s)\n",
      "Unsupported operator aten::pow encountered 1 time(s)\n",
      "Unsupported operator aten::sqrt encountered 1 time(s)\n",
      "Unsupported operator aten::lt encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 1 time(s)\n",
      "Unsupported operator aten::__or__ encountered 2 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 9887643426816\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Get a batch from the dataloader iterator\n",
    "train_loader_iter = iter(train_loader)\n",
    "batch = next(train_loader_iter)\n",
    "input_tensor = batch[\"image\"].cuda()\n",
    "\n",
    "flops = FlopCountAnalysis(wrapped_network, input_tensor)\n",
    "\n",
    "print(f\"FLOPs: {flops.total()}\")  # In raw FLOPs (can divide by 1e9 for GFLOPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1634ee8d69455ab63b9b438c5250ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da43383089c4bdeaaf0e25069129d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8594262003898621\n",
      "0.0002466946607455611\n",
      "\n",
      "0.8792238235473633\n",
      "0.009743480943143368\n",
      "\n",
      "0.7520604133605957\n",
      "0.010547947138547897\n",
      "\n",
      "0.9208256006240845\n",
      "0.005537268240004778\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m epoch_mean_dscs = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m (pbar := tqdm(train_loader)):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdefer_keysignal_with_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegmentation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapped_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/software/anaconda3/envs/segfm3d_2/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/repos/segfm3d-nora/src/training/utils.py:50\u001b[39m, in \u001b[36mdefer_keysignal_with_grad\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefer_keysignal_with_grad\u001b[39m():\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Wrap the existing defer_keysignal context manager\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdefer_keysignal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Turn on PyTorch gradient mode\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/software/anaconda3/envs/segfm3d_2/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/data/nii/data1/Analysis/GPUnet/ANALYSIS_segfm-robin/repos/segfm3d-nora/src/training/utils.py:44\u001b[39m, in \u001b[36mdefer_keysignal\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m signal.signal(signum, original_handler)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m defer_handle_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43moriginal_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdefer_handle_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from src.training.utils import compute_binary_dsc\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "n_epochs = 25\n",
    "moving_dsc = None\n",
    "all_mean_dscs = []\n",
    "i_batch = 0\n",
    "optim_network.zero_grad()\n",
    "for i_epoch in trange(n_epochs):\n",
    "    epoch_mean_dscs = []\n",
    "    for batch in (pbar := tqdm(train_loader)):\n",
    "        with defer_keysignal_with_grad():\n",
    "            targets = batch[\"segmentation\"][0][:, 0].long().cuda()\n",
    "            out = wrapped_network(batch[\"image\"].cuda())\n",
    "            # assuming deep supervision\n",
    "            pred = out[0]\n",
    "            assert pred[:, 0].shape == targets.shape\n",
    "            cent = torch.nn.functional.cross_entropy(pred, targets)\n",
    "            binary_pred = torch.sigmoid(torch.diff(pred, dim=1)[:, 0])\n",
    "            assert binary_pred.shape == targets.shape\n",
    "            eps = 1e-3\n",
    "            intersect = torch.sum(binary_pred * targets, dim=(1, 2, 3))\n",
    "            sum_pred = torch.sum(binary_pred, dim=(1, 2, 3))\n",
    "            sum_targets = torch.sum(targets, dim=(1, 2, 3))\n",
    "            mean_dsc = torch.mean((2 * intersect + eps) / (sum_pred + sum_targets + eps))\n",
    "            loss = cent + 10 * (1 - mean_dsc)\n",
    "            loss = loss / n_batches_accumulate  ## account for accumulation\n",
    "            loss.backward()\n",
    "            i_batch += 1\n",
    "            if (i_batch % n_batches_accumulate) == 0:\n",
    "                all_grad_finite = all(\n",
    "                    [\n",
    "                        (p.grad.isfinite().all().item())\n",
    "                        for group in optim_network.param_groups\n",
    "                        for p in group[\"params\"]\n",
    "                        if (p.grad is not None)\n",
    "                    ]\n",
    "                )\n",
    "                if all_grad_finite and torch.isfinite(loss).item():\n",
    "                    optim_network.step()\n",
    "                else:\n",
    "                    print(\"loss/grads not finite, not updating!\")\n",
    "                optim_network.zero_grad()\n",
    "            else:\n",
    "                pass  ## accumulate grad...\n",
    "        epoch_mean_dscs.append(mean_dsc.item())\n",
    "        if torch.isfinite(loss).item():\n",
    "            moving_dsc = (\n",
    "                mean_dsc if moving_dsc is None else (moving_dsc * 0.98 + mean_dsc * 0.02)\n",
    "            )\n",
    "        pbar.set_postfix(dict(moving_dsc=moving_dsc.item()))\n",
    "        print(mean_dsc.item())\n",
    "        print(cent.item())\n",
    "        print()\n",
    "    print(\"mean dsc\", np.nanmean(epoch_mean_dscs))\n",
    "    all_mean_dscs.extend(epoch_mean_dscs)\n",
    "    print(f\"Epoch {i_epoch}\")\n",
    "    if torch.isfinite(loss).item():\n",
    "        filename_time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")[:-3]\n",
    "        trained_chkpt[\"network_weights\"] = wrapped_network.orig_network.state_dict()\n",
    "        if torch.isfinite(loss).item():\n",
    "            torch.save(trained_chkpt, trained_chkpt_path)\n",
    "            print(f\"Checkpoint saved to {trained_chkpt_path}\")\n",
    "        current_ckpt_filename = trained_chkpt_path.replace(\n",
    "            \"/checkpoint_final.pth\", f\"/checkpoint_{i_epoch}_{filename_time_str}.pth\"\n",
    "        )\n",
    "        torch.save(trained_chkpt, current_ckpt_filename)\n",
    "        print(f\"Checkpoint also saved to {current_ckpt_filename}\")\n",
    "\n",
    "    test_results = []\n",
    "    for batch in (pbar := tqdm(test_loader)):\n",
    "        targets = batch[\"segmentation\"][:, 0].long().cuda()\n",
    "        out = wrapped_network(batch[\"image\"].cuda())\n",
    "        # assuming deep supervision\n",
    "        pred = out[0]\n",
    "        assert pred[:, 0].shape == targets.shape\n",
    "        cent = torch.nn.functional.cross_entropy(pred, targets)\n",
    "        binary_pred = torch.sigmoid(torch.diff(pred, dim=1)[:, 0])\n",
    "        assert binary_pred.shape == targets.shape\n",
    "        eps = 1e-3\n",
    "        intersect = torch.sum(binary_pred * targets, dim=(1, 2, 3))\n",
    "        sum_pred = torch.sum(binary_pred, dim=(1, 2, 3))\n",
    "        sum_targets = torch.sum(targets, dim=(1, 2, 3))\n",
    "        mean_dsc = torch.mean((2 * intersect + eps) / (sum_pred + sum_targets + eps))\n",
    "        test_results.append(\n",
    "            dict(\n",
    "                hard_dsc=compute_binary_dsc((binary_pred > 0.5), targets),\n",
    "                soft_dsc=((2 * intersect + eps) / (sum_pred + sum_targets + eps))\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "                .mean(),\n",
    "                cent=cent.item(),\n",
    "            )\n",
    "        )\n",
    "    display(pd.DataFrame(test_results).mean())\n",
    "    test_result_filepath = os.path.join(\n",
    "        os.path.split(trained_chkpt_path)[0],\n",
    "        f\"test_results_{i_epoch}_{filename_time_str}.csv\",\n",
    "    )\n",
    "    # save test results\n",
    "    pd.DataFrame(test_results).to_csv(test_result_filepath)\n",
    "    print(f\"Saved test results to {test_result_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_chkpt[\"network_weights\"] = wrapped_network.orig_network.state_dict()\n",
    "torch.save(trained_chkpt, trained_chkpt_path)\n",
    "print(f\"Checkpoint saved to {trained_chkpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_chkpt[\"network_weights\"] = wrapped_network.orig_network.state_dict()\n",
    "torch.save(trained_chkpt, trained_chkpt_path)\n",
    "print(f\"Checkpoint saved to {trained_chkpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at examples etc.\n",
    "# now try click changes, grad clip? gradient accum?\n",
    "# different sampling?\n",
    "# old loki\n",
    "# http://10.217.3.189:8888/lab?token=7fe7920caa53ef1ca025abdc8c4fdaedc532ecf340635f2f\n",
    "\n",
    "\n",
    "# nero\n",
    "# http://10.231.0.162:8888/lab?token=1e5d7331c18592a8cb58ced34b88232869e16ea21a98b4c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for batch in (pbar := tqdm(test_loader)):\n",
    "    targets = batch[\"segmentation\"][:, 0].long().cuda()\n",
    "    out = wrapped_network(batch[\"image\"].cuda())\n",
    "    # assuming deep supervision\n",
    "    pred = out[0]\n",
    "    assert pred[:, 0].shape == targets.shape\n",
    "    cent = torch.nn.functional.cross_entropy(pred, targets)\n",
    "    binary_pred = torch.sigmoid(torch.diff(pred, dim=1)[:, 0])\n",
    "    assert binary_pred.shape == targets.shape\n",
    "    eps = 1e-3\n",
    "    intersect = torch.sum(binary_pred * targets, dim=(1, 2, 3))\n",
    "    sum_pred = torch.sum(binary_pred, dim=(1, 2, 3))\n",
    "    sum_targets = torch.sum(targets, dim=(1, 2, 3))\n",
    "    mean_dsc = torch.mean((2 * intersect + eps) / (sum_pred + sum_targets + eps))\n",
    "    test_results.append(\n",
    "        dict(\n",
    "            hard_dsc=compute_binary_dsc((binary_pred > 0.5), targets),\n",
    "            soft_dsc=((2 * intersect + eps) / (sum_pred + sum_targets + eps)).cpu().numpy().mean(),\n",
    "            cent=cent.item()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_dscs = np.array(all_mean_dscs)[np.isfinite(np.array(all_mean_dscs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.correlate(finite_dscs, np.ones(100)/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(test_results).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(test_results).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segfm3d_2",
   "language": "python",
   "name": "segfm3d_2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
